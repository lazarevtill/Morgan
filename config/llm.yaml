# LLM Service Configuration (Open WebUI / Ollama API)
# Reference: https://docs.openwebui.com/getting-started/api-endpoints/

# Network configuration
host: "0.0.0.0"
port: 8001

# Open WebUI API configuration
# Set via environment variables: OLLAMA_BASE_URL and MORGAN_LLM_API_KEY
openai_api_base: ""  # Set via OLLAMA_BASE_URL environment variable
api_key: ""  # Set via MORGAN_LLM_API_KEY environment variable
model: "llama3.2:latest"  # Default chat model

# Generation parameters
max_tokens: 2048
temperature: 0.7
timeout: 30.0

# Context management
context_window: 4096
max_context_messages: 10

# System prompt
system_prompt: "You are Morgan, a helpful and friendly AI assistant. You assist with answering questions, and perform various tasks."

# Logging
log_level: "INFO"

# Embedding configuration
# Using nomic-embed-text which is optimized for Ollama embeddings
embedding_model: "nomic-embed-text:latest"

# Feature flags
enable_streaming: true
