# LLM Service Configuration (Open WebUI / Ollama API)
# Reference: https://docs.openwebui.com/getting-started/api-endpoints/

# Network configuration
host: "0.0.0.0"
port: 8001

# Open WebUI API configuration
# Chat completions: https://gpt.lazarev.cloud/ollama/v1/chat/completions (OpenAI-compatible)
# Embeddings: https://gpt.lazarev.cloud/ollama/api/embed (Ollama API)
openai_api_base: "https://gpt.lazarev.cloud/ollama/v1"
api_key: ""  # Open WebUI API key
model: "llama3.2:latest"  # Default chat model

# Generation parameters
max_tokens: 2048
temperature: 0.7
timeout: 30.0

# Context management
context_window: 4096
max_context_messages: 10

# System prompt
system_prompt: "You are Morgan, a helpful and friendly AI assistant. You assist with answering questions, and perform various tasks."

# Logging
log_level: "INFO"

# Embedding configuration
# Using nomic-embed-text which is optimized for Ollama embeddings
embedding_model: "nomic-embed-text:latest"

# Feature flags
enable_streaming: true
