# =============================================================================
# Morgan RAG Configuration
# =============================================================================
# This configuration file supports SEPARATE hosts for:
# - LLM (Language Model) - for chat/generation
# - Embedding - for vector embeddings
# - Reranking - for result reranking
# - Vector Database (Qdrant)
#
# Copy this file to .env and configure your hosts
# =============================================================================

# =============================================================================
# LLM Provider Configuration (for chat/generation)
# =============================================================================
# Your LLM endpoint (OpenAI compatible API)
# This is used for text generation, chat, and reasoning
LLM_BASE_URL=http://192.168.1.20:11434/v1
LLM_API_KEY=ollama

# LLM Model to use for generation
LLM_MODEL=qwen2.5:32b-instruct-q4_K_M

# LLM Performance settings
LLM_MAX_TOKENS=2048
LLM_TEMPERATURE=0.7
LLM_TIMEOUT=60

# =============================================================================
# Distributed LLM Configuration (Optional - for load balancing)
# =============================================================================
# Enable distributed LLM with multiple endpoints
LLM_DISTRIBUTED_ENABLED=false

# Comma-separated list of LLM endpoints for load balancing
# Example: http://192.168.1.20:11434/v1,http://192.168.1.21:11434/v1
LLM_ENDPOINTS=http://192.168.1.20:11434/v1,http://192.168.1.21:11434/v1

# Load balancing strategy: round_robin, random, least_loaded
LLM_LOAD_BALANCING_STRATEGY=round_robin

# Health check interval in seconds
LLM_HEALTH_CHECK_INTERVAL=60

# =============================================================================
# Embedding Provider Configuration (SEPARATE from LLM)
# =============================================================================
# Embedding endpoint - CAN BE DIFFERENT from LLM endpoint
# This allows you to run embeddings on a dedicated host
EMBEDDING_BASE_URL=http://192.168.1.22:11434

# Alternative: Use OLLAMA_HOST if embedding is on Ollama
# OLLAMA_HOST=192.168.1.22:11434

# Embedding model (must be available on EMBEDDING_BASE_URL)
EMBEDDING_MODEL=nomic-embed-text

# Embedding dimensions (depends on model)
# - nomic-embed-text: 768
# - qwen3-embedding:latest: 4096
# - all-MiniLM-L6-v2: 384
EMBEDDING_DIMENSIONS=768

# Local fallback model (when remote unavailable)
EMBEDDING_LOCAL_MODEL=all-MiniLM-L6-v2

# Force remote embeddings only (no local fallback)
EMBEDDING_FORCE_REMOTE=false

# Batch size for embedding operations
EMBEDDING_BATCH_SIZE=100

# Device for local embeddings (cpu, cuda, mps)
EMBEDDING_DEVICE=cpu

# Use instruction prefixes for better relevance (22% improvement)
EMBEDDING_USE_INSTRUCTIONS=true

# =============================================================================
# Reranking Provider Configuration (SEPARATE from LLM and Embedding)
# =============================================================================
# Reranking endpoint - runs on dedicated GPU host
RERANKING_ENABLED=true
RERANKING_ENDPOINT=http://192.168.1.23:8081/rerank

# Reranking model for local fallback
RERANKING_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Force remote reranking only
RERANKING_FORCE_REMOTE=false

# Reranking timeout
RERANKING_TIMEOUT=30

# =============================================================================
# Vector Database (Qdrant)
# =============================================================================
# Qdrant connection
QDRANT_URL=http://localhost:6333

# Optional API key for Qdrant
QDRANT_API_KEY=

# Collection names
QDRANT_DEFAULT_COLLECTION=morgan_knowledge
QDRANT_MEMORY_COLLECTION=morgan_memories
# Optional override for hierarchical collection (defaults to <QDRANT_DEFAULT_COLLECTION>_hierarchical)
QDRANT_HIERARCHICAL_COLLECTION=

# =============================================================================
# Morgan System Settings
# =============================================================================
# Data directory for Morgan's data storage
MORGAN_DATA_DIR=./data

# Log level (DEBUG, INFO, WARNING, ERROR)
MORGAN_LOG_LEVEL=INFO

# Enable debug mode
MORGAN_DEBUG=false

# Maximum context length for LLM
MORGAN_MAX_CONTEXT=8192

# Maximum tokens for responses
MORGAN_MAX_RESPONSE_TOKENS=2048

# =============================================================================
# Performance Settings
# =============================================================================
# Number of worker processes
MORGAN_WORKERS=4

# Cache size (number of items)
MORGAN_CACHE_SIZE=1000

# Cache TTL in seconds
MORGAN_CACHE_TTL=3600

# Search result limits
MORGAN_MAX_SEARCH_RESULTS=50
MORGAN_DEFAULT_SEARCH_RESULTS=10

# Minimum search score threshold (0.0-1.0)
MORGAN_MIN_SEARCH_SCORE=0.7

# =============================================================================
# Web Interface Settings
# =============================================================================
# Web server host and port
MORGAN_HOST=0.0.0.0
MORGAN_PORT=8080

# API server port
MORGAN_API_PORT=8000

# Enable web interface
MORGAN_WEB_ENABLED=true

# API documentation
MORGAN_DOCS_ENABLED=true

# CORS origins (comma-separated, * for all)
MORGAN_CORS_ORIGINS=*

# =============================================================================
# Security Settings
# =============================================================================
# API key for Morgan API (leave empty for no auth)
MORGAN_API_KEY=

# Session secret (CHANGE IN PRODUCTION!)
MORGAN_SESSION_SECRET=change-me-in-production-use-strong-secret

# Feature flags
MORGAN_ALLOW_FILE_UPLOAD=true
MORGAN_ALLOW_URL_INGESTION=true
MORGAN_ALLOW_CODE_EXECUTION=false

# =============================================================================
# Memory & Learning Settings
# =============================================================================
# Conversation memory
MORGAN_MEMORY_ENABLED=true
MORGAN_MEMORY_MAX_CONVERSATIONS=1000
MORGAN_MEMORY_MAX_TURNS_PER_CONVERSATION=100

# Learning from feedback
MORGAN_LEARNING_ENABLED=true
MORGAN_FEEDBACK_WEIGHT=0.1

# Knowledge graph
MORGAN_KNOWLEDGE_GRAPH_ENABLED=true
MORGAN_ENTITY_EXTRACTION_ENABLED=true

# =============================================================================
# Document Processing Settings
# =============================================================================
# Chunk size for document splitting
MORGAN_CHUNK_SIZE=1000

# Chunk overlap for better context
MORGAN_CHUNK_OVERLAP=200

# Maximum file size for upload (MB)
MORGAN_MAX_FILE_SIZE=100

# Supported file types (comma-separated)
MORGAN_SUPPORTED_TYPES=pdf,docx,txt,md,html,py,js,ts,go,java,cpp,c,h,json,yaml,yml

# Web scraping settings
MORGAN_SCRAPE_DEPTH=3
MORGAN_SCRAPE_DELAY=1
MORGAN_SCRAPE_TIMEOUT=30

# =============================================================================
# External Services (Optional)
# =============================================================================
# Redis for caching
REDIS_URL=redis://localhost:6379

# PostgreSQL for metadata (optional)
DATABASE_URL=

# Elasticsearch for full-text search (optional)
ELASTICSEARCH_URL=

# Consul for service discovery (optional)
CONSUL_ENABLED=false
CONSUL_HTTP_ADDR=http://localhost:8500

# HuggingFace token (for private models)
HUGGINGFACE_HUB_TOKEN=

# =============================================================================
# Monitoring & Analytics (Optional)
# =============================================================================
# Enable metrics collection
MORGAN_METRICS_ENABLED=true

# Prometheus metrics port
MORGAN_METRICS_PORT=9000

# Health check interval (seconds)
MORGAN_HEALTH_CHECK_INTERVAL=30

# Analytics retention (days)
MORGAN_ANALYTICS_RETENTION=90

# Grafana admin password
GRAFANA_PASSWORD=admin

# Qdrant log level
QDRANT_LOG_LEVEL=INFO

# =============================================================================
# Development Settings
# =============================================================================
# Enable development mode
MORGAN_DEV_MODE=false

# Auto-reload on code changes
MORGAN_AUTO_RELOAD=false

# Enable request logging
MORGAN_REQUEST_LOGGING=false

# =============================================================================
# 6-Host Distributed Architecture Example
# =============================================================================
# Host 1 (192.168.1.10): Morgan Core + Qdrant + Redis
# Host 2 (192.168.1.11): Background Services + Monitoring
# Host 3 (192.168.1.20): RTX 3090 #1 - Main LLM
# Host 4 (192.168.1.21): RTX 3090 #2 - Main LLM (load balanced)
# Host 5 (192.168.1.22): RTX 4070 - Embeddings + Fast LLM
# Host 6 (192.168.1.23): RTX 2060 - Reranking
#
# Example configuration for this setup:
# LLM_BASE_URL=http://192.168.1.20:11434/v1
# LLM_DISTRIBUTED_ENABLED=true
# LLM_ENDPOINTS=http://192.168.1.20:11434/v1,http://192.168.1.21:11434/v1
# EMBEDDING_BASE_URL=http://192.168.1.22:11434
# RERANKING_ENDPOINT=http://192.168.1.23:8081/rerank
# QDRANT_URL=http://192.168.1.10:6333
# REDIS_URL=redis://192.168.1.10:6379
# =============================================================================
