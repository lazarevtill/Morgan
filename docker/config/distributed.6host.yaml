# Morgan 6-Host Distributed Architecture Configuration
# 100% Self-Hosted - No API Keys Required
#
# This configuration is for the full 6-host distributed setup:
# - Host 1 (192.168.1.10): Morgan Core + Qdrant + Redis
# - Host 2 (192.168.1.11): Background Services + Monitoring
# - Host 3 (192.168.1.20): Main LLM #1 (RTX 3090)
# - Host 4 (192.168.1.21): Main LLM #2 (RTX 3090)
# - Host 5 (192.168.1.22): Embeddings + Fast LLM (RTX 4070)
# - Host 6 (192.168.1.23): Reranking (RTX 2060)
#
# Copy and customize IP addresses for your network:
#   cp distributed.6host.yaml distributed.local.yaml

# Global settings
settings:
  health_check_interval: 60
  default_timeout: 30.0
  load_balancing_strategy: round_robin
  enable_failover: true
  max_retries: 3

# Model cache configuration
# Models are downloaded once to this location and reused on subsequent starts
# This avoids re-downloading weights on each container restart
model_cache:
  # Base directory for all cached model weights (inside container)
  base_dir: "/app/models"
  # sentence-transformers cache (embeddings, rerankers)
  sentence_transformers_home: "/app/models/sentence-transformers"
  # Hugging Face cache
  hf_home: "/app/models/huggingface"
  # Ollama models directory (mounted from host)
  ollama_models: "/root/.ollama/models"
  # Pre-download models on startup (first run may take a few minutes)
  preload_on_startup: true

# LLM Configuration (Ollama)
llm:
  main_model: "qwen2.5:32b-instruct-q4_K_M"
  fast_model: "qwen2.5:7b-instruct-q5_K_M"
  temperature: 0.7
  max_tokens: 2048

# Embedding Configuration (Self-Hosted)
embeddings:
  # nomic-embed-text via Ollama
  model: "nomic-embed-text"
  dimensions: 768
  local_fallback_model: "all-MiniLM-L6-v2"
  batch_size: 100

# Reranking Configuration (Self-Hosted)
reranking:
  # CrossEncoder via sentence-transformers
  model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  rerank_weight: 0.6
  original_weight: 0.4
  top_k: 20

# Host definitions for 6-host architecture
hosts:
# CPU Host 1 - Morgan Core Orchestrator
- host_id: "cpu-orchestrator-1"
  address: "192.168.1.10"
  port: 8080
  role: "orchestrator"
  description: "Primary Morgan core with Qdrant and Redis"
  models: []

# CPU Host 2 - Background Services
- host_id: "cpu-orchestrator-2"
  address: "192.168.1.11"
  port: 8080
  role: "orchestrator"
  description: "Background services and monitoring"
  models: []

# GPU Host 3 - Main LLM Primary (RTX 3090)
- host_id: "llm-primary"
  address: "192.168.1.20"
  port: 11434
  role: "main_llm"
  gpu_model: "RTX 3090"
  gpu_vram_gb: 24.0
  api_path: "/v1"
  description: "Primary LLM inference via Ollama"
  models:
  - "qwen2.5:32b-instruct-q4_K_M"

# GPU Host 4 - Main LLM Secondary (RTX 3090)
- host_id: "llm-secondary"
  address: "192.168.1.21"
  port: 11434
  role: "main_llm"
  gpu_model: "RTX 3090"
  gpu_vram_gb: 24.0
  api_path: "/v1"
  description: "Secondary LLM inference via Ollama"
  models:
  - "qwen2.5:32b-instruct-q4_K_M"

# GPU Host 5 - Embeddings + Fast LLM (RTX 4070)
- host_id: "embeddings-host"
  address: "192.168.1.22"
  port: 11434
  role: "embeddings"
  gpu_model: "RTX 4070"
  gpu_vram_gb: 12.0
  api_path: "/v1"
  description: "Embedding generation via Ollama"
  models:
  - "nomic-embed-text"
  - "qwen2.5:7b-instruct-q5_K_M"

# GPU Host 6 - Reranking (RTX 2060)
- host_id: "reranking-host"
  address: "192.168.1.23"
  port: 8080
  role: "reranking"
  gpu_model: "RTX 2060"
  gpu_vram_gb: 6.0
  api_path: "/rerank"
  description: "Document reranking via sentence-transformers"
  models:
  - "cross-encoder/ms-marco-MiniLM-L-6-v2"

# Redis Configuration (on Host 1)
redis:
  host: "192.168.1.10"
  port: 6379
  db: 0
  password: ""
  prefix: "morgan:"

# Vector Database (Qdrant on Host 1)
qdrant:
  host: "192.168.1.10"
  port: 6333
  grpc_port: 6334

# Monitoring Configuration (on Host 2)
monitoring:
  prometheus:
    host: "192.168.1.11"
    port: 9090
  grafana:
    host: "192.168.1.11"
    port: 3000
